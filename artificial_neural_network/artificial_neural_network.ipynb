{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9pYPjXu1c29j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yellowbrick.classifier import ConfusionMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BtFpMcErdOrP"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d4nlCsfnXU7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iMTTgH1gdRR2"
      },
      "outputs": [],
      "source": [
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(iris.data, iris.target,\n",
        "                                                                  test_size = 0.3,\n",
        "                                                                  random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YzGgO1sZdUnQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.67412160\n",
            "Iteration 2, loss = 1.65236067\n",
            "Iteration 3, loss = 1.63201455\n",
            "Iteration 4, loss = 1.61300240\n",
            "Iteration 5, loss = 1.59510998\n",
            "Iteration 6, loss = 1.57791143\n",
            "Iteration 7, loss = 1.56137368\n",
            "Iteration 8, loss = 1.54536418\n",
            "Iteration 9, loss = 1.52978198\n",
            "Iteration 10, loss = 1.51449724\n",
            "Iteration 11, loss = 1.49931433\n",
            "Iteration 12, loss = 1.48410811\n",
            "Iteration 13, loss = 1.46886328\n",
            "Iteration 14, loss = 1.45359336\n",
            "Iteration 15, loss = 1.43825937\n",
            "Iteration 16, loss = 1.42282730\n",
            "Iteration 17, loss = 1.40731909\n",
            "Iteration 18, loss = 1.39182770\n",
            "Iteration 19, loss = 1.37639418\n",
            "Iteration 20, loss = 1.36105189\n",
            "Iteration 21, loss = 1.34575523\n",
            "Iteration 22, loss = 1.33058253\n",
            "Iteration 23, loss = 1.31562445\n",
            "Iteration 24, loss = 1.30091024\n",
            "Iteration 25, loss = 1.28641025\n",
            "Iteration 26, loss = 1.27207147\n",
            "Iteration 27, loss = 1.25789553\n",
            "Iteration 28, loss = 1.24385901\n",
            "Iteration 29, loss = 1.22998867\n",
            "Iteration 30, loss = 1.21634779\n",
            "Iteration 31, loss = 1.20287756\n",
            "Iteration 32, loss = 1.18957095\n",
            "Iteration 33, loss = 1.17642978\n",
            "Iteration 34, loss = 1.16346653\n",
            "Iteration 35, loss = 1.15069025\n",
            "Iteration 36, loss = 1.13812843\n",
            "Iteration 37, loss = 1.12584226\n",
            "Iteration 38, loss = 1.11371151\n",
            "Iteration 39, loss = 1.10175945\n",
            "Iteration 40, loss = 1.09000500\n",
            "Iteration 41, loss = 1.07844245\n",
            "Iteration 42, loss = 1.06705954\n",
            "Iteration 43, loss = 1.05581729\n",
            "Iteration 44, loss = 1.04472225\n",
            "Iteration 45, loss = 1.03381425\n",
            "Iteration 46, loss = 1.02305148\n",
            "Iteration 47, loss = 1.01249122\n",
            "Iteration 48, loss = 1.00213325\n",
            "Iteration 49, loss = 0.99196664\n",
            "Iteration 50, loss = 0.98203365\n",
            "Iteration 51, loss = 0.97226545\n",
            "Iteration 52, loss = 0.96268570\n",
            "Iteration 53, loss = 0.95328129\n",
            "Iteration 54, loss = 0.94406954\n",
            "Iteration 55, loss = 0.93508681\n",
            "Iteration 56, loss = 0.92631924\n",
            "Iteration 57, loss = 0.91771207\n",
            "Iteration 58, loss = 0.90924768\n",
            "Iteration 59, loss = 0.90093676\n",
            "Iteration 60, loss = 0.89278773\n",
            "Iteration 61, loss = 0.88481949\n",
            "Iteration 62, loss = 0.87709662\n",
            "Iteration 63, loss = 0.86972957\n",
            "Iteration 64, loss = 0.86275090\n",
            "Iteration 65, loss = 0.85625721\n",
            "Iteration 66, loss = 0.85005803\n",
            "Iteration 67, loss = 0.84403939\n",
            "Iteration 68, loss = 0.83811011\n",
            "Iteration 69, loss = 0.83250266\n",
            "Iteration 70, loss = 0.82721950\n",
            "Iteration 71, loss = 0.82208570\n",
            "Iteration 72, loss = 0.81711407\n",
            "Iteration 73, loss = 0.81231428\n",
            "Iteration 74, loss = 0.80760476\n",
            "Iteration 75, loss = 0.80298292\n",
            "Iteration 76, loss = 0.79845088\n",
            "Iteration 77, loss = 0.79397286\n",
            "Iteration 78, loss = 0.78959712\n",
            "Iteration 79, loss = 0.78526297\n",
            "Iteration 80, loss = 0.78098617\n",
            "Iteration 81, loss = 0.77675889\n",
            "Iteration 82, loss = 0.77257816\n",
            "Iteration 83, loss = 0.76842206\n",
            "Iteration 84, loss = 0.76432312\n",
            "Iteration 85, loss = 0.76026657\n",
            "Iteration 86, loss = 0.75627181\n",
            "Iteration 87, loss = 0.75233828\n",
            "Iteration 88, loss = 0.74846509\n",
            "Iteration 89, loss = 0.74461536\n",
            "Iteration 90, loss = 0.74076496\n",
            "Iteration 91, loss = 0.73695942\n",
            "Iteration 92, loss = 0.73319826\n",
            "Iteration 93, loss = 0.72948073\n",
            "Iteration 94, loss = 0.72580588\n",
            "Iteration 95, loss = 0.72208285\n",
            "Iteration 96, loss = 0.71833075\n",
            "Iteration 97, loss = 0.71453775\n",
            "Iteration 98, loss = 0.71075254\n",
            "Iteration 99, loss = 0.70698765\n",
            "Iteration 100, loss = 0.70316843\n",
            "Iteration 101, loss = 0.69931204\n",
            "Iteration 102, loss = 0.69546336\n",
            "Iteration 103, loss = 0.69153175\n",
            "Iteration 104, loss = 0.68744163\n",
            "Iteration 105, loss = 0.68343607\n",
            "Iteration 106, loss = 0.67934706\n",
            "Iteration 107, loss = 0.67491081\n",
            "Iteration 108, loss = 0.67061825\n",
            "Iteration 109, loss = 0.66636870\n",
            "Iteration 110, loss = 0.66227944\n",
            "Iteration 111, loss = 0.65844573\n",
            "Iteration 112, loss = 0.65483283\n",
            "Iteration 113, loss = 0.65136116\n",
            "Iteration 114, loss = 0.64822021\n",
            "Iteration 115, loss = 0.64522920\n",
            "Iteration 116, loss = 0.64249361\n",
            "Iteration 117, loss = 0.63983020\n",
            "Iteration 118, loss = 0.63722529\n",
            "Iteration 119, loss = 0.63463467\n",
            "Iteration 120, loss = 0.63211799\n",
            "Iteration 121, loss = 0.62965171\n",
            "Iteration 122, loss = 0.62726128\n",
            "Iteration 123, loss = 0.62487601\n",
            "Iteration 124, loss = 0.62251746\n",
            "Iteration 125, loss = 0.62018960\n",
            "Iteration 126, loss = 0.61787313\n",
            "Iteration 127, loss = 0.61555621\n",
            "Iteration 128, loss = 0.61330795\n",
            "Iteration 129, loss = 0.61110160\n",
            "Iteration 130, loss = 0.60897022\n",
            "Iteration 131, loss = 0.60691621\n",
            "Iteration 132, loss = 0.60488964\n",
            "Iteration 133, loss = 0.60287719\n",
            "Iteration 134, loss = 0.60087251\n",
            "Iteration 135, loss = 0.59889564\n",
            "Iteration 136, loss = 0.59691497\n",
            "Iteration 137, loss = 0.59493908\n",
            "Iteration 138, loss = 0.59297653\n",
            "Iteration 139, loss = 0.59105040\n",
            "Iteration 140, loss = 0.58913190\n",
            "Iteration 141, loss = 0.58722078\n",
            "Iteration 142, loss = 0.58531717\n",
            "Iteration 143, loss = 0.58342820\n",
            "Iteration 144, loss = 0.58156058\n",
            "Iteration 145, loss = 0.57970555\n",
            "Iteration 146, loss = 0.57786565\n",
            "Iteration 147, loss = 0.57605220\n",
            "Iteration 148, loss = 0.57427135\n",
            "Iteration 149, loss = 0.57250628\n",
            "Iteration 150, loss = 0.57073915\n",
            "Iteration 151, loss = 0.56897324\n",
            "Iteration 152, loss = 0.56721013\n",
            "Iteration 153, loss = 0.56544910\n",
            "Iteration 154, loss = 0.56370232\n",
            "Iteration 155, loss = 0.56196405\n",
            "Iteration 156, loss = 0.56023405\n",
            "Iteration 157, loss = 0.55852358\n",
            "Iteration 158, loss = 0.55682660\n",
            "Iteration 159, loss = 0.55513230\n",
            "Iteration 160, loss = 0.55344400\n",
            "Iteration 161, loss = 0.55176233\n",
            "Iteration 162, loss = 0.55009047\n",
            "Iteration 163, loss = 0.54843264\n",
            "Iteration 164, loss = 0.54678470\n",
            "Iteration 165, loss = 0.54514254\n",
            "Iteration 166, loss = 0.54350745\n",
            "Iteration 167, loss = 0.54188727\n",
            "Iteration 168, loss = 0.54027290\n",
            "Iteration 169, loss = 0.53866524\n",
            "Iteration 170, loss = 0.53707163\n",
            "Iteration 171, loss = 0.53548703\n",
            "Iteration 172, loss = 0.53390897\n",
            "Iteration 173, loss = 0.53233876\n",
            "Iteration 174, loss = 0.53077241\n",
            "Iteration 175, loss = 0.52920960\n",
            "Iteration 176, loss = 0.52765003\n",
            "Iteration 177, loss = 0.52609338\n",
            "Iteration 178, loss = 0.52454001\n",
            "Iteration 179, loss = 0.52299018\n",
            "Iteration 180, loss = 0.52144385\n",
            "Iteration 181, loss = 0.51989998\n",
            "Iteration 182, loss = 0.51835886\n",
            "Iteration 183, loss = 0.51681818\n",
            "Iteration 184, loss = 0.51527786\n",
            "Iteration 185, loss = 0.51374020\n",
            "Iteration 186, loss = 0.51220409\n",
            "Iteration 187, loss = 0.51066887\n",
            "Iteration 188, loss = 0.50913349\n",
            "Iteration 189, loss = 0.50759788\n",
            "Iteration 190, loss = 0.50606245\n",
            "Iteration 191, loss = 0.50452939\n",
            "Iteration 192, loss = 0.50299642\n",
            "Iteration 193, loss = 0.50146343\n",
            "Iteration 194, loss = 0.49993029\n",
            "Iteration 195, loss = 0.49839605\n",
            "Iteration 196, loss = 0.49686074\n",
            "Iteration 197, loss = 0.49532648\n",
            "Iteration 198, loss = 0.49379205\n",
            "Iteration 199, loss = 0.49225678\n",
            "Iteration 200, loss = 0.49072048\n",
            "Iteration 201, loss = 0.48918336\n",
            "Iteration 202, loss = 0.48764427\n",
            "Iteration 203, loss = 0.48610347\n",
            "Iteration 204, loss = 0.48456411\n",
            "Iteration 205, loss = 0.48302444\n",
            "Iteration 206, loss = 0.48148474\n",
            "Iteration 207, loss = 0.47994463\n",
            "Iteration 208, loss = 0.47840234\n",
            "Iteration 209, loss = 0.47685765\n",
            "Iteration 210, loss = 0.47531035\n",
            "Iteration 211, loss = 0.47376048\n",
            "Iteration 212, loss = 0.47221005\n",
            "Iteration 213, loss = 0.47065664\n",
            "Iteration 214, loss = 0.46909629\n",
            "Iteration 215, loss = 0.46753098\n",
            "Iteration 216, loss = 0.46596121\n",
            "Iteration 217, loss = 0.46439091\n",
            "Iteration 218, loss = 0.46281606\n",
            "Iteration 219, loss = 0.46123869\n",
            "Iteration 220, loss = 0.45966000\n",
            "Iteration 221, loss = 0.45807706\n",
            "Iteration 222, loss = 0.45649004\n",
            "Iteration 223, loss = 0.45490458\n",
            "Iteration 224, loss = 0.45332823\n",
            "Iteration 225, loss = 0.45175261\n",
            "Iteration 226, loss = 0.45017919\n",
            "Iteration 227, loss = 0.44860442\n",
            "Iteration 228, loss = 0.44703576\n",
            "Iteration 229, loss = 0.44546891\n",
            "Iteration 230, loss = 0.44391200\n",
            "Iteration 231, loss = 0.44235126\n",
            "Iteration 232, loss = 0.44079050\n",
            "Iteration 233, loss = 0.43923144\n",
            "Iteration 234, loss = 0.43767647\n",
            "Iteration 235, loss = 0.43614215\n",
            "Iteration 236, loss = 0.43461277\n",
            "Iteration 237, loss = 0.43308540\n",
            "Iteration 238, loss = 0.43155965\n",
            "Iteration 239, loss = 0.43003531\n",
            "Iteration 240, loss = 0.42851279\n",
            "Iteration 241, loss = 0.42699616\n",
            "Iteration 242, loss = 0.42548113\n",
            "Iteration 243, loss = 0.42396765\n",
            "Iteration 244, loss = 0.42245570\n",
            "Iteration 245, loss = 0.42094531\n",
            "Iteration 246, loss = 0.41943743\n",
            "Iteration 247, loss = 0.41793328\n",
            "Iteration 248, loss = 0.41643004\n",
            "Iteration 249, loss = 0.41493373\n",
            "Iteration 250, loss = 0.41343893\n",
            "Iteration 251, loss = 0.41194404\n",
            "Iteration 252, loss = 0.41045160\n",
            "Iteration 253, loss = 0.40896502\n",
            "Iteration 254, loss = 0.40748478\n",
            "Iteration 255, loss = 0.40599868\n",
            "Iteration 256, loss = 0.40451202\n",
            "Iteration 257, loss = 0.40302537\n",
            "Iteration 258, loss = 0.40153413\n",
            "Iteration 259, loss = 0.40001615\n",
            "Iteration 260, loss = 0.39844325\n",
            "Iteration 261, loss = 0.39687435\n",
            "Iteration 262, loss = 0.39524363\n",
            "Iteration 263, loss = 0.39355118\n",
            "Iteration 264, loss = 0.39178384\n",
            "Iteration 265, loss = 0.38993508\n",
            "Iteration 266, loss = 0.38802396\n",
            "Iteration 267, loss = 0.38604265\n",
            "Iteration 268, loss = 0.38400101\n",
            "Iteration 269, loss = 0.38186653\n",
            "Iteration 270, loss = 0.37970818\n",
            "Iteration 271, loss = 0.37755226\n",
            "Iteration 272, loss = 0.37540190\n",
            "Iteration 273, loss = 0.37325139\n",
            "Iteration 274, loss = 0.37106684\n",
            "Iteration 275, loss = 0.36889250\n",
            "Iteration 276, loss = 0.36672940\n",
            "Iteration 277, loss = 0.36457943\n",
            "Iteration 278, loss = 0.36244415\n",
            "Iteration 279, loss = 0.36032494\n",
            "Iteration 280, loss = 0.35822185\n",
            "Iteration 281, loss = 0.35613401\n",
            "Iteration 282, loss = 0.35406952\n",
            "Iteration 283, loss = 0.35202531\n",
            "Iteration 284, loss = 0.35000176\n",
            "Iteration 285, loss = 0.34800997\n",
            "Iteration 286, loss = 0.34611687\n",
            "Iteration 287, loss = 0.34429956\n",
            "Iteration 288, loss = 0.34251554\n",
            "Iteration 289, loss = 0.34080639\n",
            "Iteration 290, loss = 0.33915535\n",
            "Iteration 291, loss = 0.33755036\n",
            "Iteration 292, loss = 0.33600497\n",
            "Iteration 293, loss = 0.33447888\n",
            "Iteration 294, loss = 0.33294487\n",
            "Iteration 295, loss = 0.33139389\n",
            "Iteration 296, loss = 0.32983616\n",
            "Iteration 297, loss = 0.32827719\n",
            "Iteration 298, loss = 0.32672238\n",
            "Iteration 299, loss = 0.32517859\n",
            "Iteration 300, loss = 0.32364011\n",
            "Iteration 301, loss = 0.32210480\n",
            "Iteration 302, loss = 0.32056927\n",
            "Iteration 303, loss = 0.31902572\n",
            "Iteration 304, loss = 0.31746402\n",
            "Iteration 305, loss = 0.31589969\n",
            "Iteration 306, loss = 0.31433335\n",
            "Iteration 307, loss = 0.31276783\n",
            "Iteration 308, loss = 0.31118523\n",
            "Iteration 309, loss = 0.30951051\n",
            "Iteration 310, loss = 0.30780385\n",
            "Iteration 311, loss = 0.30606845\n",
            "Iteration 312, loss = 0.30427301\n",
            "Iteration 313, loss = 0.30252766\n",
            "Iteration 314, loss = 0.30080814\n",
            "Iteration 315, loss = 0.29906906\n",
            "Iteration 316, loss = 0.29743065\n",
            "Iteration 317, loss = 0.29576084\n",
            "Iteration 318, loss = 0.29404396\n",
            "Iteration 319, loss = 0.29224392\n",
            "Iteration 320, loss = 0.29032202\n",
            "Iteration 321, loss = 0.28846502\n",
            "Iteration 322, loss = 0.28662032\n",
            "Iteration 323, loss = 0.28478649\n",
            "Iteration 324, loss = 0.28296035\n",
            "Iteration 325, loss = 0.28114622\n",
            "Iteration 326, loss = 0.27933885\n",
            "Iteration 327, loss = 0.27754203\n",
            "Iteration 328, loss = 0.27575761\n",
            "Iteration 329, loss = 0.27398565\n",
            "Iteration 330, loss = 0.27222494\n",
            "Iteration 331, loss = 0.27047377\n",
            "Iteration 332, loss = 0.26873082\n",
            "Iteration 333, loss = 0.26699559\n",
            "Iteration 334, loss = 0.26526864\n",
            "Iteration 335, loss = 0.26355132\n",
            "Iteration 336, loss = 0.26185069\n",
            "Iteration 337, loss = 0.26017705\n",
            "Iteration 338, loss = 0.25851678\n",
            "Iteration 339, loss = 0.25686963\n",
            "Iteration 340, loss = 0.25523514\n",
            "Iteration 341, loss = 0.25361288\n",
            "Iteration 342, loss = 0.25200469\n",
            "Iteration 343, loss = 0.25041104\n",
            "Iteration 344, loss = 0.24882894\n",
            "Iteration 345, loss = 0.24725904\n",
            "Iteration 346, loss = 0.24570187\n",
            "Iteration 347, loss = 0.24415766\n",
            "Iteration 348, loss = 0.24262628\n",
            "Iteration 349, loss = 0.24110740\n",
            "Iteration 350, loss = 0.23960039\n",
            "Iteration 351, loss = 0.23810466\n",
            "Iteration 352, loss = 0.23661984\n",
            "Iteration 353, loss = 0.23514576\n",
            "Iteration 354, loss = 0.23368245\n",
            "Iteration 355, loss = 0.23223005\n",
            "Iteration 356, loss = 0.23078870\n",
            "Iteration 357, loss = 0.22935845\n",
            "Iteration 358, loss = 0.22793925\n",
            "Iteration 359, loss = 0.22653102\n",
            "Iteration 360, loss = 0.22513344\n",
            "Iteration 361, loss = 0.22374630\n",
            "Iteration 362, loss = 0.22236941\n",
            "Iteration 363, loss = 0.22100268\n",
            "Iteration 364, loss = 0.21964608\n",
            "Iteration 365, loss = 0.21829961\n",
            "Iteration 366, loss = 0.21696327\n",
            "Iteration 367, loss = 0.21563706\n",
            "Iteration 368, loss = 0.21432093\n",
            "Iteration 369, loss = 0.21301473\n",
            "Iteration 370, loss = 0.21171835\n",
            "Iteration 371, loss = 0.21043249\n",
            "Iteration 372, loss = 0.20915787\n",
            "Iteration 373, loss = 0.20789282\n",
            "Iteration 374, loss = 0.20663727\n",
            "Iteration 375, loss = 0.20539117\n",
            "Iteration 376, loss = 0.20415445\n",
            "Iteration 377, loss = 0.20292707\n",
            "Iteration 378, loss = 0.20170897\n",
            "Iteration 379, loss = 0.20050017\n",
            "Iteration 380, loss = 0.19930052\n",
            "Iteration 381, loss = 0.19810998\n",
            "Iteration 382, loss = 0.19692851\n",
            "Iteration 383, loss = 0.19575606\n",
            "Iteration 384, loss = 0.19459258\n",
            "Iteration 385, loss = 0.19343801\n",
            "Iteration 386, loss = 0.19229232\n",
            "Iteration 387, loss = 0.19115544\n",
            "Iteration 388, loss = 0.19002732\n",
            "Iteration 389, loss = 0.18890793\n",
            "Iteration 390, loss = 0.18779721\n",
            "Iteration 391, loss = 0.18669519\n",
            "Iteration 392, loss = 0.18560176\n",
            "Iteration 393, loss = 0.18451687\n",
            "Iteration 394, loss = 0.18344047\n",
            "Iteration 395, loss = 0.18237251\n",
            "Iteration 396, loss = 0.18131294\n",
            "Iteration 397, loss = 0.18026171\n",
            "Iteration 398, loss = 0.17921877\n",
            "Iteration 399, loss = 0.17818408\n",
            "Iteration 400, loss = 0.17715760\n",
            "Iteration 401, loss = 0.17613927\n",
            "Iteration 402, loss = 0.17512904\n",
            "Iteration 403, loss = 0.17412688\n",
            "Iteration 404, loss = 0.17313273\n",
            "Iteration 405, loss = 0.17214654\n",
            "Iteration 406, loss = 0.17116828\n",
            "Iteration 407, loss = 0.17019788\n",
            "Iteration 408, loss = 0.16923531\n",
            "Iteration 409, loss = 0.16828052\n",
            "Iteration 410, loss = 0.16733345\n",
            "Iteration 411, loss = 0.16639407\n",
            "Iteration 412, loss = 0.16546233\n",
            "Iteration 413, loss = 0.16453817\n",
            "Iteration 414, loss = 0.16362154\n",
            "Iteration 415, loss = 0.16271242\n",
            "Iteration 416, loss = 0.16181073\n",
            "Iteration 417, loss = 0.16091644\n",
            "Iteration 418, loss = 0.16002950\n",
            "Iteration 419, loss = 0.15914986\n",
            "Iteration 420, loss = 0.15827748\n",
            "Iteration 421, loss = 0.15741230\n",
            "Iteration 422, loss = 0.15655428\n",
            "Iteration 423, loss = 0.15570338\n",
            "Iteration 424, loss = 0.15485953\n",
            "Iteration 425, loss = 0.15402270\n",
            "Iteration 426, loss = 0.15319284\n",
            "Iteration 427, loss = 0.15236989\n",
            "Iteration 428, loss = 0.15155382\n",
            "Iteration 429, loss = 0.15074457\n",
            "Iteration 430, loss = 0.14994210\n",
            "Iteration 431, loss = 0.14914635\n",
            "Iteration 432, loss = 0.14835728\n",
            "Iteration 433, loss = 0.14757484\n",
            "Iteration 434, loss = 0.14679899\n",
            "Iteration 435, loss = 0.14602967\n",
            "Iteration 436, loss = 0.14526683\n",
            "Iteration 437, loss = 0.14451044\n",
            "Iteration 438, loss = 0.14376044\n",
            "Iteration 439, loss = 0.14301678\n",
            "Iteration 440, loss = 0.14227941\n",
            "Iteration 441, loss = 0.14154830\n",
            "Iteration 442, loss = 0.14082338\n",
            "Iteration 443, loss = 0.14010583\n",
            "Iteration 444, loss = 0.13939437\n",
            "Iteration 445, loss = 0.13868895\n",
            "Iteration 446, loss = 0.13798967\n",
            "Iteration 447, loss = 0.13729667\n",
            "Iteration 448, loss = 0.13660950\n",
            "Iteration 449, loss = 0.13592809\n",
            "Iteration 450, loss = 0.13525240\n",
            "Iteration 451, loss = 0.13458275\n",
            "Iteration 452, loss = 0.13391887\n",
            "Iteration 453, loss = 0.13326065\n",
            "Iteration 454, loss = 0.13260808\n",
            "Iteration 455, loss = 0.13196154\n",
            "Iteration 456, loss = 0.13132044\n",
            "Iteration 457, loss = 0.13068479\n",
            "Iteration 458, loss = 0.13005457\n",
            "Iteration 459, loss = 0.12943000\n",
            "Iteration 460, loss = 0.12881095\n",
            "Iteration 461, loss = 0.12819709\n",
            "Iteration 462, loss = 0.12758848\n",
            "Iteration 463, loss = 0.12698520\n",
            "Iteration 464, loss = 0.12638702\n",
            "Iteration 465, loss = 0.12579391\n",
            "Iteration 466, loss = 0.12520587\n",
            "Iteration 467, loss = 0.12462293\n",
            "Iteration 468, loss = 0.12404501\n",
            "Iteration 469, loss = 0.12347213\n",
            "Iteration 470, loss = 0.12290419\n",
            "Iteration 471, loss = 0.12234111\n",
            "Iteration 472, loss = 0.12178286\n",
            "Iteration 473, loss = 0.12122939\n",
            "Iteration 474, loss = 0.12068064\n",
            "Iteration 475, loss = 0.12013659\n",
            "Iteration 476, loss = 0.11959718\n",
            "Iteration 477, loss = 0.11906240\n",
            "Iteration 478, loss = 0.11853218\n",
            "Iteration 479, loss = 0.11800649\n",
            "Iteration 480, loss = 0.11748547\n",
            "Iteration 481, loss = 0.11696915\n",
            "Iteration 482, loss = 0.11645791\n",
            "Iteration 483, loss = 0.11595124\n",
            "Iteration 484, loss = 0.11544885\n",
            "Iteration 485, loss = 0.11495074\n",
            "Iteration 486, loss = 0.11445692\n",
            "Iteration 487, loss = 0.11396741\n",
            "Iteration 488, loss = 0.11348216\n",
            "Iteration 489, loss = 0.11300111\n",
            "Iteration 490, loss = 0.11252420\n",
            "Iteration 491, loss = 0.11205132\n",
            "Iteration 492, loss = 0.11158247\n",
            "Iteration 493, loss = 0.11111758\n",
            "Iteration 494, loss = 0.11065662\n",
            "Iteration 495, loss = 0.11019953\n",
            "Iteration 496, loss = 0.10974629\n",
            "Iteration 497, loss = 0.10929720\n",
            "Iteration 498, loss = 0.10885194\n",
            "Iteration 499, loss = 0.10841035\n",
            "Iteration 500, loss = 0.10797253\n",
            "Iteration 501, loss = 0.10753856\n",
            "Iteration 502, loss = 0.10710839\n",
            "Iteration 503, loss = 0.10668149\n",
            "Iteration 504, loss = 0.10625824\n",
            "Iteration 505, loss = 0.10583860\n",
            "Iteration 506, loss = 0.10542253\n",
            "Iteration 507, loss = 0.10501025\n",
            "Iteration 508, loss = 0.10460151\n",
            "Iteration 509, loss = 0.10419618\n",
            "Iteration 510, loss = 0.10379422\n",
            "Iteration 511, loss = 0.10339560\n",
            "Iteration 512, loss = 0.10300037\n",
            "Iteration 513, loss = 0.10260841\n",
            "Iteration 514, loss = 0.10221983\n",
            "Iteration 515, loss = 0.10183503\n",
            "Iteration 516, loss = 0.10145333\n",
            "Iteration 517, loss = 0.10107482\n",
            "Iteration 518, loss = 0.10069955\n",
            "Iteration 519, loss = 0.10032773\n",
            "Iteration 520, loss = 0.09995893\n",
            "Iteration 521, loss = 0.09959285\n",
            "Iteration 522, loss = 0.09922990\n",
            "Iteration 523, loss = 0.09886987\n",
            "Iteration 524, loss = 0.09851276\n",
            "Iteration 525, loss = 0.09815868\n",
            "Iteration 526, loss = 0.09780756\n",
            "Iteration 527, loss = 0.09745929\n",
            "Iteration 528, loss = 0.09711391\n",
            "Iteration 529, loss = 0.09677169\n",
            "Iteration 530, loss = 0.09643244\n",
            "Iteration 531, loss = 0.09609595\n",
            "Iteration 532, loss = 0.09576217\n",
            "Iteration 533, loss = 0.09543107\n",
            "Iteration 534, loss = 0.09510262\n",
            "Iteration 535, loss = 0.09477678\n",
            "Iteration 536, loss = 0.09445372\n",
            "Iteration 537, loss = 0.09413322\n",
            "Iteration 538, loss = 0.09381533\n",
            "Iteration 539, loss = 0.09349996\n",
            "Iteration 540, loss = 0.09318703\n",
            "Iteration 541, loss = 0.09287654\n",
            "Iteration 542, loss = 0.09256843\n",
            "Iteration 543, loss = 0.09226279\n",
            "Iteration 544, loss = 0.09195970\n",
            "Iteration 545, loss = 0.09165941\n",
            "Iteration 546, loss = 0.09136201\n",
            "Iteration 547, loss = 0.09106713\n",
            "Iteration 548, loss = 0.09077455\n",
            "Iteration 549, loss = 0.09048421\n",
            "Iteration 550, loss = 0.09019612\n",
            "Iteration 551, loss = 0.08991072\n",
            "Iteration 552, loss = 0.08962773\n",
            "Iteration 553, loss = 0.08934695\n",
            "Iteration 554, loss = 0.08906838\n",
            "Iteration 555, loss = 0.08879199\n",
            "Iteration 556, loss = 0.08851773\n",
            "Iteration 557, loss = 0.08824562\n",
            "Iteration 558, loss = 0.08797552\n",
            "Iteration 559, loss = 0.08770751\n",
            "Iteration 560, loss = 0.08744151\n",
            "Iteration 561, loss = 0.08717754\n",
            "Iteration 562, loss = 0.08691559\n",
            "Iteration 563, loss = 0.08665604\n",
            "Iteration 564, loss = 0.08639844\n",
            "Iteration 565, loss = 0.08614297\n",
            "Iteration 566, loss = 0.08588947\n",
            "Iteration 567, loss = 0.08563790\n",
            "Iteration 568, loss = 0.08538817\n",
            "Iteration 569, loss = 0.08514027\n",
            "Iteration 570, loss = 0.08489429\n",
            "Iteration 571, loss = 0.08465009\n",
            "Iteration 572, loss = 0.08440760\n",
            "Iteration 573, loss = 0.08416692\n",
            "Iteration 574, loss = 0.08392802\n",
            "Iteration 575, loss = 0.08369096\n",
            "Iteration 576, loss = 0.08345588\n",
            "Iteration 577, loss = 0.08322261\n",
            "Iteration 578, loss = 0.08299101\n",
            "Iteration 579, loss = 0.08276103\n",
            "Iteration 580, loss = 0.08253273\n",
            "Iteration 581, loss = 0.08230610\n",
            "Iteration 582, loss = 0.08208105\n",
            "Iteration 583, loss = 0.08185756\n",
            "Iteration 584, loss = 0.08163572\n",
            "Iteration 585, loss = 0.08141544\n",
            "Iteration 586, loss = 0.08119690\n",
            "Iteration 587, loss = 0.08097993\n",
            "Iteration 588, loss = 0.08076453\n",
            "Iteration 589, loss = 0.08055060\n",
            "Iteration 590, loss = 0.08033812\n",
            "Iteration 591, loss = 0.08012711\n",
            "Iteration 592, loss = 0.07991770\n",
            "Iteration 593, loss = 0.07970983\n",
            "Iteration 594, loss = 0.07950337\n",
            "Iteration 595, loss = 0.07929836\n",
            "Iteration 596, loss = 0.07909474\n",
            "Iteration 597, loss = 0.07889257\n",
            "Iteration 598, loss = 0.07869192\n",
            "Iteration 599, loss = 0.07849263\n",
            "Iteration 600, loss = 0.07829468\n",
            "Iteration 601, loss = 0.07809820\n",
            "Iteration 602, loss = 0.07790322\n",
            "Iteration 603, loss = 0.07770955\n",
            "Iteration 604, loss = 0.07751720\n",
            "Iteration 605, loss = 0.07732614\n",
            "Iteration 606, loss = 0.07713656\n",
            "Iteration 607, loss = 0.07694838\n",
            "Iteration 608, loss = 0.07676153\n",
            "Iteration 609, loss = 0.07657587\n",
            "Iteration 610, loss = 0.07639142\n",
            "Iteration 611, loss = 0.07620823\n",
            "Iteration 612, loss = 0.07602631\n",
            "Iteration 613, loss = 0.07584567\n",
            "Iteration 614, loss = 0.07566620\n",
            "Iteration 615, loss = 0.07548797\n",
            "Iteration 616, loss = 0.07531081\n",
            "Iteration 617, loss = 0.07513481\n",
            "Iteration 618, loss = 0.07495996\n",
            "Iteration 619, loss = 0.07478622\n",
            "Iteration 620, loss = 0.07461357\n",
            "Iteration 621, loss = 0.07444202\n",
            "Iteration 622, loss = 0.07427178\n",
            "Iteration 623, loss = 0.07410263\n",
            "Iteration 624, loss = 0.07393455\n",
            "Iteration 625, loss = 0.07376752\n",
            "Iteration 626, loss = 0.07360153\n",
            "Iteration 627, loss = 0.07343657\n",
            "Iteration 628, loss = 0.07327263\n",
            "Iteration 629, loss = 0.07310999\n",
            "Iteration 630, loss = 0.07294847\n",
            "Iteration 631, loss = 0.07278796\n",
            "Iteration 632, loss = 0.07262847\n",
            "Iteration 633, loss = 0.07246998\n",
            "Iteration 634, loss = 0.07231246\n",
            "Iteration 635, loss = 0.07215589\n",
            "Iteration 636, loss = 0.07200025\n",
            "Iteration 637, loss = 0.07184555\n",
            "Iteration 638, loss = 0.07169176\n",
            "Iteration 639, loss = 0.07153891\n",
            "Iteration 640, loss = 0.07138696\n",
            "Iteration 641, loss = 0.07123592\n",
            "Iteration 642, loss = 0.07108583\n",
            "Iteration 643, loss = 0.07093684\n",
            "Iteration 644, loss = 0.07078872\n",
            "Iteration 645, loss = 0.07064148\n",
            "Iteration 646, loss = 0.07049510\n",
            "Iteration 647, loss = 0.07034959\n",
            "Iteration 648, loss = 0.07020492\n",
            "Iteration 649, loss = 0.07006109\n",
            "Iteration 650, loss = 0.06991808\n",
            "Iteration 651, loss = 0.06977589\n",
            "Iteration 652, loss = 0.06963469\n",
            "Iteration 653, loss = 0.06949436\n",
            "Iteration 654, loss = 0.06935484\n",
            "Iteration 655, loss = 0.06921612\n",
            "Iteration 656, loss = 0.06907819\n",
            "Iteration 657, loss = 0.06894104\n",
            "Iteration 658, loss = 0.06880465\n",
            "Iteration 659, loss = 0.06866902\n",
            "Iteration 660, loss = 0.06853415\n",
            "Iteration 661, loss = 0.06840001\n",
            "Iteration 662, loss = 0.06826661\n",
            "Iteration 663, loss = 0.06813394\n",
            "Iteration 664, loss = 0.06800199\n",
            "Iteration 665, loss = 0.06787075\n",
            "Iteration 666, loss = 0.06774021\n",
            "Iteration 667, loss = 0.06761037\n",
            "Iteration 668, loss = 0.06748122\n",
            "Iteration 669, loss = 0.06735275\n",
            "Iteration 670, loss = 0.06722496\n",
            "Iteration 671, loss = 0.06709785\n",
            "Iteration 672, loss = 0.06697140\n",
            "Iteration 673, loss = 0.06684560\n",
            "Iteration 674, loss = 0.06672046\n",
            "Iteration 675, loss = 0.06659597\n",
            "Iteration 676, loss = 0.06647211\n",
            "Iteration 677, loss = 0.06634889\n",
            "Iteration 678, loss = 0.06622630\n",
            "Iteration 679, loss = 0.06610434\n",
            "Iteration 680, loss = 0.06598299\n",
            "Iteration 681, loss = 0.06586226\n",
            "Iteration 682, loss = 0.06574214\n",
            "Iteration 683, loss = 0.06562261\n",
            "Iteration 684, loss = 0.06550369\n",
            "Iteration 685, loss = 0.06538536\n",
            "Iteration 686, loss = 0.06526782\n",
            "Iteration 687, loss = 0.06515091\n",
            "Iteration 688, loss = 0.06503462\n",
            "Iteration 689, loss = 0.06491896\n",
            "Iteration 690, loss = 0.06480389\n",
            "Iteration 691, loss = 0.06468940\n",
            "Iteration 692, loss = 0.06457548\n",
            "Iteration 693, loss = 0.06446216\n",
            "Iteration 694, loss = 0.06434943\n",
            "Iteration 695, loss = 0.06423740\n",
            "Iteration 696, loss = 0.06412597\n",
            "Iteration 697, loss = 0.06401509\n",
            "Iteration 698, loss = 0.06390476\n",
            "Iteration 699, loss = 0.06379497\n",
            "Iteration 700, loss = 0.06368571\n",
            "Iteration 701, loss = 0.06357699\n",
            "Iteration 702, loss = 0.06346880\n",
            "Iteration 703, loss = 0.06336113\n",
            "Iteration 704, loss = 0.06325397\n",
            "Iteration 705, loss = 0.06314733\n",
            "Iteration 706, loss = 0.06304118\n",
            "Iteration 707, loss = 0.06293554\n",
            "Iteration 708, loss = 0.06283039\n",
            "Iteration 709, loss = 0.06272574\n",
            "Iteration 710, loss = 0.06262157\n",
            "Iteration 711, loss = 0.06251788\n",
            "Iteration 712, loss = 0.06241467\n",
            "Iteration 713, loss = 0.06231194\n",
            "Iteration 714, loss = 0.06220967\n",
            "Iteration 715, loss = 0.06210787\n",
            "Iteration 716, loss = 0.06200654\n",
            "Iteration 717, loss = 0.06190567\n",
            "Iteration 718, loss = 0.06180535\n",
            "Iteration 719, loss = 0.06170549\n",
            "Iteration 720, loss = 0.06160608\n",
            "Iteration 721, loss = 0.06150712\n",
            "Iteration 722, loss = 0.06140861\n",
            "Iteration 723, loss = 0.06131059\n",
            "Iteration 724, loss = 0.06121317\n",
            "Iteration 725, loss = 0.06111626\n",
            "Iteration 726, loss = 0.06101980\n",
            "Iteration 727, loss = 0.06092379\n",
            "Iteration 728, loss = 0.06082821\n",
            "Iteration 729, loss = 0.06073305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(6, 7), max_iter=10000, verbose=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = MLPClassifier(verbose = True, hidden_layer_sizes=(6,7), max_iter = 10000)\n",
        "modelo.fit(X_treinamento, y_treinamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xTPP10L0dW0a"
      },
      "outputs": [],
      "source": [
        "previsoes = modelo.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujjGAPZ62M1L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
              "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0,\n",
              "       0])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VywcBaJ42SOO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
              "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0,\n",
              "       0])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K5qgDEB5db0K"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9777777777777777"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WNgHYNWide9b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'MLPClassifier Confusion Matrix'}, xlabel='Predicted Class', ylabel='True Class'>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusao = ConfusionMatrix(modelo)\n",
        "confusao.fit(X_treinamento, y_treinamento)\n",
        "confusao.score(X_teste, y_teste)\n",
        "confusao.poof()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7ccb692d2b9a7eb31aee4d66a327ca30c1c8b803c148696b42050ce9accbbccc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
